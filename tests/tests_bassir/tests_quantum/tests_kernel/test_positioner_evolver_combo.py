from torch import randn
from bassir.models.quantum.rydberg import RydbergEvolver
from bassir.models.quantum.positioner import Positioner
from bassir.utils.qutils import get_default_register_topology
import pytest
import torch


def test_positioner():
    """
    Test that the positioner works and is differentiable with respect to its input x.

    We check that when we compute the loss from the outputted mask, gradients propagate back to x.
    Since x may become non-leaf during forward computations, we call x.retain_grad() explicitly.
    """
    batch_size, dim = 4, 4
    n_qubits = 2
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    x = randn((batch_size, dim)).to(device)

    traps = get_default_register_topology(topology="all_to_all", n_qubits=n_qubits)
    # Instantiate Positioner
    positioner = Positioner(traps, dim=dim).to(device)
    mask = positioner(x)

    assert mask.shape == (batch_size, n_qubits), (f"Shape mismatch, got mask.shape = {mask.shape}, "
                                                  f"should have been {(batch_size, n_qubits)}")

    # Assert that every row has at least one active element.
    for i in range(10):
        x = randn((batch_size, dim)).to(device)
        mask1, mask2 = positioner(x), positioner(x)

        # Assert that two runs of the same input yield the same mask:
        assert torch.equal(mask1, mask2), "The positioner's output yields are different given the same input."

        # Sum each row: result is a tensor of shape (batch_size)
        row_sums = mask1.sum(dim=-1)
        # Assert that every row has at least one active element.
        assert torch.all(row_sums >= 1), "Some rows have no active bit."

    # For our simple test, define a scalar loss.
    loss = mask.sum()
    loss.backward()

    # Check that positioner's gradients.
    n_zeros = 0
    n_params = 0
    for param in positioner.parameters():
        n_params += 1
        assert not torch.isnan(param.grad).any(), "Some of the positioner's gradients are None."
        if torch.all(0 == param.grad):
            n_zeros += 1

    assert n_zeros != n_params, "Positioner's gradients are always zero."


def test_positioner_evolver_combo():
    """
    Test that the combination positioner evolver (using the Positioner and RydbergEvolver) works and
    is differentiable with respect to its input x.

    We check that when we compute the loss from the outputted state, gradients propagate back to x.
    Since x may become non-leaf during forward computations, we call x.retain_grad() explicitly.
    """
    batch_size, dim = 4, 12
    n_qubits = 2
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Create random inputs with requires_grad=True
    x = randn((batch_size, dim)).to(device)

    traps = get_default_register_topology(topology="all_to_all", n_qubits=n_qubits)

    # Instantiate Positioner, RydbergEvolver, and the BassirKernel.
    positioner = Positioner(traps, dim=dim).to(device)
    evolver = RydbergEvolver(traps=traps, dim=dim).to(device)

    psi_output_batch = evolver(x, positioner(x))

    # For our simple test, define a scalar loss.
    loss = (psi_output_batch.abs() ** 2).sum()
    loss.backward()

    # Check that positioner's gradients are computed.
    n_params = 0
    n_zeros = 0
    for param in positioner.parameters():
        n_params += 1
        assert not torch.isnan(param.grad).any(), "Some of the positioner's gradients are None."
        if torch.all(0 == param.grad):
            n_zeros += 1
    assert n_zeros != n_params, "Positioner's gradients are always zero."

    n_params = 0
    n_zeros = 0
    for param in evolver.parameters():
        n_params += 1
        assert not torch.isnan(param.grad).any(), "Some of the evolver's gradients are None."
        if torch.all(0 == param.grad):
            n_zeros += 1
    assert n_zeros != n_params, "Evolver's gradients are always zero."


def test_combo_shape_and_normalization():
    """
    Verify that for a given random batch and the masks generated by the positioner, the evolved state:
      (i) has the expected shape (batch_size, 2^(n_qubits)), and
      (ii) is approximately normalized.
    """
    n_qubits = 2
    batch_size = 3
    dim = 10
    traps = get_default_register_topology(topology="all_to_all", n_qubits=n_qubits)
    positioner = Positioner(traps, dim=dim)
    evolver = RydbergEvolver(traps=traps, dim=dim)

    x = torch.randn(batch_size, dim)
    mask_batch = positioner(x)

    psi_out = evolver(x, mask_batch)
    expected_shape = (batch_size, 2 ** n_qubits)
    assert psi_out.shape == expected_shape, f"Expected shape {expected_shape}, got {psi_out.shape}"
    norms = torch.linalg.norm(psi_out, dim=1)
    assert torch.allclose(norms, torch.ones_like(norms), atol=1e-4), f"State norms not close to 1: {norms}"


if __name__ == '__main__':
    pytest.main([__file__])
