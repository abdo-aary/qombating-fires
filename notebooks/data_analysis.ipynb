{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180ad541-11be-41ac-8aff-79ba3eba2d62",
   "metadata": {},
   "source": [
    "### Data Analysis\n",
    "#### This notebook is here to analyse the data we preprocessed, in order to focus on the area that are mainly concerned by wildfires.\n",
    "\n",
    "#### If you want to get the final dataset without the explanation, run the file **/data/prep/data_analysis.py**.\n",
    "\n",
    "#### Before running any cells make sure that the file **dataset_pre_analysis.csv** is in the folder **storage/dataset**, if not you have to run **/data/prep/data_preprocessing.py**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aec2e4b-5131-48a7-bd31-19c35f3c4e92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m current_dir  \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[1;32m      6\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_pre_analysis.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# convert IS_FIRE to float\u001b[39;00m\n\u001b[1;32m     10\u001b[0m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIS_FIRE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIS_FIRE\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pasqal/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pasqal/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pasqal/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/pasqal/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:334\u001b[0m, in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "current_dir  = os.getcwd()\n",
    "dataset_path = os.path.join(current_dir, '..', 'storage', 'dataset',\"dataset_pre_analysis.csv\")\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# convert IS_FIRE to float\n",
    "dataset[\"IS_FIRE\"] = dataset[\"IS_FIRE\"].astype(float)\n",
    "\n",
    "# Group by CELL_LAT and CELL_LON and select those who have IS_FIRE = 1.0\n",
    "df = dataset.groupby([\"CELL_LAT\", \"CELL_LON\"])\n",
    "number_cell = len(df)\n",
    "fire_cells = df['IS_FIRE'].max().reset_index()\n",
    "fire_cells = fire_cells[fire_cells['IS_FIRE'] == 1.0]\n",
    "#count the fire cells\n",
    "count_fire_cells = fire_cells.shape[0]\n",
    "\n",
    "print(\"Only\" ,count_fire_cells,\"out of\",number_cell,\"are on fire at least one time between 2010 and 2021.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef09267-67b5-40c0-b401-616840dba6e2",
   "metadata": {},
   "source": [
    "#### As only 1278 out of 6789 are on fire at least one time, we have to reduce the dimensionality of the dataset in order to focus on the area that are concerned by wildfires.\n",
    "\n",
    "#### Let's see it on a map, next we'll plot every cell of the dataset and every one that as a red dot in it means that there is at least one wildfire in the cell in between 2010 and 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848508a-8a1e-48b8-b569-5f2913de9cd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data.view.plot_map import *\n",
    "url = \"https://naciscdn.org/naturalearth/50m/cultural/ne_50m_admin_1_states_provinces.zip\"\n",
    "canada = gpd.read_file(url)\n",
    "\n",
    "# Filtrer pour obtenir uniquement le Québec\n",
    "quebec = canada[canada[\"name\"] == \"Québec\"]\n",
    "fire_cells_coords = list(fire_cells[['CELL_LON', 'CELL_LAT']].itertuples(index=False, name=None))\n",
    "fig,ax = plot_qc_map(quebec)\n",
    "for lon, lat in fire_cells_coords:\n",
    "    ax.scatter(lon, lat, color='red', s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf4540-6eaa-44c4-8d2b-9fb4168e02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fire_path = os.path.join(current_dir, '..', 'storage', 'dataset',\"dataset_fire_cleaned.csv\")\n",
    "df_fire = pd.read_csv(dataset_fire_path)\n",
    "# Convertir les colonnes en float (en s'assurant qu'il n'y a pas d'erreurs de conversion)\n",
    "df_fire[\"LONGITUDE\"] = pd.to_numeric(df_fire[\"LONGITUDE\"], errors='coerce')\n",
    "df_fire[\"LATITUDE\"] = pd.to_numeric(df_fire[\"LATITUDE\"], errors='coerce')\n",
    "\n",
    "# Filtrer les données\n",
    "filtered_df = df_fire[(df_fire[\"LONGITUDE\"] >= -79.5) & (df_fire[\"LONGITUDE\"] <= -64) &\n",
    "                 (df_fire[\"LATITUDE\"] >= 45.25) & (df_fire[\"LATITUDE\"] <= 52)]\n",
    "\n",
    "# Compter les résultats\n",
    "count = len(filtered_df)\n",
    "countfull = len(df_fire)\n",
    "print(f\"{round(count/countfull*100,2)}% of fires are contained -79.5 and -58.75 and the latitudes 45.25 and 52\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c0192-15b2-4964-a10c-dbc94f0faac9",
   "metadata": {},
   "source": [
    "#### As 91.84% of wildfires are contained -79.5 and -58.75 and the latitudes 45.25 and 52, so we will restrict our dataset to theses latitudes and longitudes, we also have to recalculate the coordinates since we changed the coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bb239-cf53-477c-9c7b-7731ecbfbfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = dataset[(dataset[\"CELL_LON\"] >= -79.5) & (dataset[\"CELL_LON\"] <= -64) &\n",
    "                 (dataset[\"CELL_LAT\"] >= 45.25) & (dataset[\"CELL_LAT\"] <= 52)]\n",
    "filtered_dataset[[\"COORDINATES_LAT\",\"COORDINATES_LON\" ]] = ((filtered_dataset[[\"CELL_LAT\", \"CELL_LON\"]]\n",
    "                                                            - [45.25,-79.5]) / [0.25,0.25]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e214cedc-005f-4cbb-b29e-67a7943edcb1",
   "metadata": {},
   "source": [
    "#### We will now plot the area we are focusing on, it is the one contained in the green rectangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3501f15d-b528-4ea7-b07b-04b53f35700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.view.plot_map import *\n",
    "fire_cells_coords = list(fire_cells[['CELL_LON', 'CELL_LAT']].itertuples(index=False, name=None))\n",
    "fig,ax = plot_qc_map(quebec)\n",
    "for lon, lat in fire_cells_coords:\n",
    "    ax.scatter(lon, lat, color='red', s=10)\n",
    "ax.plot([-79.625,-79.625,-63.875,-63.875,-79.625],[52.125,45.125,45.125,52.125,52.125],color='green',linestyle='-',linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f94b5-7a0f-4f44-a279-64c3b87fea2c",
   "metadata": {},
   "source": [
    "#### We will now erase the datapoints that are in the ocean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf49cc1-323a-42c2-aa23-f8d2dcebd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.view.pixel_label import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e6bac-295d-438d-8bd1-044a62d5a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2019-08-01'\n",
    "filtered_dataset_date = filtered_dataset[filtered_dataset['DATE'] == date]\n",
    "# Créer une géométrie de points avec latitudes et longitudes\n",
    "filtered_dataset_date['IN_OCEAN'] = filtered_dataset_date.apply(lambda row: is_cell_in_ocean(row['CELL_LAT'], row['CELL_LON'], ocean_gdf), axis=1)\n",
    "\n",
    "\n",
    "# Extraire uniquement les points situés dans l'océan\n",
    "points_in_ocean = filtered_dataset_date[filtered_dataset_date['IN_OCEAN']]\n",
    "\n",
    "# Extraire la liste des latitudes et longitudes des points dans l'océan\n",
    "lat_lon_list_ocean = points_in_ocean[['CELL_LAT', 'CELL_LON']].values.tolist()\n",
    "lat_lon_set_ocean = set(map(tuple, lat_lon_list_ocean))\n",
    "\n",
    "# Supprimer les lignes de filtered_dataset qui ont des coordonnées dans l'océan\n",
    "filtered_dataset_no_ocean = filtered_dataset[~filtered_dataset[['CELL_LAT', 'CELL_LON']].apply(tuple, axis=1).isin(lat_lon_set_ocean)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65541b03-1114-4ebd-8c75-871bcfde1914",
   "metadata": {},
   "source": [
    "#### Let's see which points contained in the green rectangle are in the ocean. They are ploted in the map as the blue points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d821f-0dd1-4b82-aa19-48edddcb7dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data.view.plot_map import *\n",
    "fire_cells_coords = list(fire_cells[['CELL_LON', 'CELL_LAT']].itertuples(index=False, name=None))\n",
    "fig,ax = plot_qc_map(quebec)\n",
    "for coo in lat_lon_list_ocean :\n",
    "    ax.scatter(coo[1], coo[0], color='blue', s=10)\n",
    "ax.plot([-79.625,-79.625,-63.875,-63.875,-79.625],[52.125,45.125,45.125,52.125,52.125],color='green',linestyle='-',linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0b476-57f2-470c-b9c9-81a3c456eb66",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### We will now erase the datapoints that aren't in Québec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd92eb-5190-43d6-ac46-a9e42c2ec3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2019-08-01'\n",
    "filtered_dataset_no_ocean_date = filtered_dataset_no_ocean[filtered_dataset_no_ocean['DATE'] == date]\n",
    "# Créer une géométrie de points avec latitudes et longitudes\n",
    "filtered_dataset_no_ocean_date['IN_QC'] = filtered_dataset_no_ocean_date.apply(lambda row: is_cell_in_quebec(row['CELL_LAT'], row['CELL_LON'], quebec), axis=1)\n",
    "\n",
    "\n",
    "# Extraire uniquement les points situés en dehors du québec\n",
    "points_out_quebec = filtered_dataset_no_ocean_date[filtered_dataset_no_ocean_date['IN_QC']==False]\n",
    "\n",
    "# Extraire la liste des latitudes et longitudes des points dans l'océan\n",
    "lat_lon_list_notqc = points_out_quebec[['CELL_LAT', 'CELL_LON']].values.tolist()\n",
    "lat_lon_set_notqc = set(map(tuple, lat_lon_list_notqc))\n",
    "\n",
    "# Supprimer les lignes de filtered_dataset qui ont des coordonnées dans l'océan\n",
    "filtered_dataset_no_ocean_qconly = filtered_dataset_no_ocean[~filtered_dataset_no_ocean[['CELL_LAT', 'CELL_LON']].apply(tuple, axis=1).isin(lat_lon_set_notqc)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8c5db-5903-4dd5-8c60-b8cec02ed52c",
   "metadata": {},
   "source": [
    "#### Let's see which points contained in the green rectangle are not in Québec. They are ploted in the map as the black points. Some may are not in other territory other than Québec, but they are in the ocean and we couldn't erase them with the previous functions, but we can do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ecd4b-8f6e-46f6-8fc3-aa8873e05a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data.view.plot_map import *\n",
    "fire_cells_coords = list(fire_cells[['CELL_LON', 'CELL_LAT']].itertuples(index=False, name=None))\n",
    "fig,ax = plot_qc_map(quebec)\n",
    "for coo in lat_lon_list_notqc :\n",
    "    ax.scatter(coo[1], coo[0], color='black', s=10)\n",
    "ax.plot([-79.625,-79.625,-63.875,-63.875,-79.625],[52.125,45.125,45.125,52.125,52.125],color='green',linestyle='-',linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51fb989-dd5c-48c4-8a10-4cd66a693bb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### So in a nutshell now the dataset is are the cells contained in the green rectangle without the pixels that are within other territory than Québec and the ocean, so it is the following map with the orange pixels for each day between 2010 and 2021, without the blue and black ones. And then the geographical analysis will be finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28c7af-9163-49b8-9904-d4b113820b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data.view.plot_map import *\n",
    "fire_cells_coords = list(fire_cells[['CELL_LON', 'CELL_LAT']].itertuples(index=False, name=None))\n",
    "fig,ax = plot_qc_map(quebec)\n",
    "for coo in lat_lon_list_notqc :\n",
    "    ax.scatter(coo[1], coo[0], color='black', s=10)\n",
    "ax.plot([-79.625,-79.625,-63.875,-63.875,-79.625],[52.125,45.125,45.125,52.125,52.125],color='green',linestyle='-',linewidth=3)\n",
    "for coo in lat_lon_list_ocean :\n",
    "    ax.scatter(coo[1], coo[0], color='blue', s=10)\n",
    "filtered_dataset_no_ocean_qconly_date = filtered_dataset_no_ocean_qconly[filtered_dataset_no_ocean_qconly['DATE'] == date]\n",
    "lat_lon_list= filtered_dataset_no_ocean_qconly_date[['CELL_LAT', 'CELL_LON']].values.tolist()\n",
    "lat_lon_set = set(map(tuple, lat_lon_list))\n",
    "for coo in lat_lon_set :\n",
    "    ax.scatter(coo[1], coo[0], color='orange', s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e235bf21-cafc-4c0e-933f-263a045cacbb",
   "metadata": {},
   "source": [
    "#### We know naturraly that some time periods aren't concerned about forest fire, like the winter season for example. We will see for each year which period have no fire at all and erase it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c07d9c-62ea-4b5b-8646-63f1b5ddee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_fire[\n",
    "    (df_fire[\"CELL_LON\"] >= -79.5) & (df_fire[\"CELL_LON\"] <= -64) &\n",
    "    (df_fire[\"CELL_LAT\"] >= 45.25) & (df_fire[\"CELL_LAT\"] <= 52)\n",
    "]\n",
    "\n",
    "# Convertir les dates en datetime pour les manipulations\n",
    "df_filtered[\"REP_DATE\"] = pd.to_datetime(df_filtered[\"REP_DATE\"])\n",
    "\n",
    "# Extraire l'année, le mois, le jour\n",
    "df_filtered[\"year\"] = df_filtered[\"REP_DATE\"].dt.year\n",
    "df_filtered[\"month\"] = df_filtered[\"REP_DATE\"].dt.month\n",
    "df_filtered[\"day\"] = df_filtered[\"REP_DATE\"].dt.day\n",
    "\n",
    "# Trouver la date la plus tôt et la plus tard pour chaque année\n",
    "earliest_per_year = df_filtered.groupby(\"year\")[\"REP_DATE\"].min()\n",
    "latest_per_year = df_filtered.groupby(\"year\")[\"REP_DATE\"].max()\n",
    "\n",
    "# Trouver le mois et le jour les plus tôt et les plus tard parmi les dates minimales et maximales\n",
    "earliest_month_day = earliest_per_year.dt.strftime('%m-%d').min()\n",
    "latest_month_day = latest_per_year.dt.strftime('%m-%d').max()\n",
    "\n",
    "# Résultat\n",
    "print(\"Global earliest month-day:\", earliest_month_day)\n",
    "print(\"Global latest month-day:\", latest_month_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44663be-f84d-405a-8c9d-abe0c10e9f63",
   "metadata": {},
   "source": [
    "#### We see that before 3<sup>rd</sup> March and after 7<sup>th</sup> December there is no fire at all, so we erase the period between the 7<sup>th</sup> December and 31<sup>st</sup> December, and the 1<sup>st</sup> January and 3<sup>rd</sup> March."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3213dd-cb1a-45f0-88fe-744052195987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assurez-vous que la colonne DATE est de type datetime\n",
    "filtered_dataset_no_ocean_qconly[\"DATE\"] = pd.to_datetime(filtered_dataset_no_ocean_qconly[\"DATE\"], errors='coerce')\n",
    "\n",
    "# Créer un masque pour exclure les dates entre le 7 décembre et le 31 décembre\n",
    "mask_december_exclude = (filtered_dataset_no_ocean_qconly[\"DATE\"].dt.month == 12) & (filtered_dataset_no_ocean_qconly[\"DATE\"].dt.day > 7)\n",
    "\n",
    "# Créer un masque pour exclure les dates entre le 1er janvier et le 11 mars (y compris le 1er janvier, mais excluant le 11 mars)\n",
    "mask_jan_march_exclude = ((filtered_dataset_no_ocean_qconly[\"DATE\"].dt.month == 1) | (filtered_dataset_no_ocean_qconly[\"DATE\"].dt.month == 3)) & \\\n",
    "                          ((filtered_dataset_no_ocean_qconly[\"DATE\"].dt.day >= 1) & (filtered_dataset_no_ocean_qconly[\"DATE\"].dt.day <= 10))\n",
    "\n",
    "# Créer un masque pour exclure toutes les dates avant le 12 mars\n",
    "mask_before_march_12 = (filtered_dataset_no_ocean_qconly[\"DATE\"].dt.month == 1) | \\\n",
    "                        ((filtered_dataset_no_ocean_qconly[\"DATE\"].dt.month == 2) & (filtered_dataset_no_ocean_qconly[\"DATE\"].dt.day <= 28)) | \\\n",
    "                        ((filtered_dataset_no_ocean_qconly[\"DATE\"].dt.month == 3) & (filtered_dataset_no_ocean_qconly[\"DATE\"].dt.day <= 10))\n",
    "\n",
    "# Filtrer les données selon ces critères\n",
    "filtered_data = filtered_dataset_no_ocean_qconly[~(mask_december_exclude | mask_jan_march_exclude | mask_before_march_12)]\n",
    "# filtered_data.to_csv(\"dataset_end_analysis.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef1ddc-3047-4c3a-8c88-d23b88144a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filtered_data.sort_values(by=['CELL_LAT', 'CELL_LON', 'DATE'])\n",
    "\n",
    "# Ajouter une colonne indiquant si le pixel sera en feu au prochain temps\n",
    "filtered_data['IS_FIRE_NEXT_DAY'] = filtered_data.groupby(['CELL_LAT', 'CELL_LON'])['IS_FIRE'].shift(-1)\n",
    "\n",
    "# Remplacer les NaN (qui apparaissent pour le dernier temps de chaque pixel) par 0\n",
    "filtered_data['IS_FIRE_NEXT_DAY'] = filtered_data['IS_FIRE_NEXT_DAY'].fillna(0).astype(int)\n",
    "\n",
    "dataset_analysis_path = os.path.join(current_dir, '..', 'storage', 'dataset',\"dataset_end_analysis.csv\")\n",
    "filtered_data.to_csv(dataset_analysis_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2bf2c-807e-44d7-ae50-0eeac700dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorized_haversine_formula(latitudes1, longitudes1, latitudes2, longitudes2):\n",
    "#     lat1, lon1 = np.radians(latitudes1), np.radians(longitudes1)\n",
    "#     lat2, lon2 = np.radians(latitudes2), np.radians(longitudes2)\n",
    "#     d_haversine = 2 * 6378 * np.arcsin(np.sqrt(np.sin((lat2 - lat1) / 2) ** 2 +\n",
    "#                                                np.cos(lat1) * np.cos(lat2) * np.sin((lon2 - lon1) / 2) ** 2))\n",
    "#     return d_haversine\n",
    "# def vectorized_cell_width_length(latitudes, longitudes, incr_lat, incr_lon):\n",
    "#     # Calculer les origines des points et les points adjacents\n",
    "#     origine_lat = latitudes - incr_lat / 2\n",
    "#     origine_lon = longitudes - incr_lon / 2\n",
    "#     point1_lat = latitudes + incr_lat / 2\n",
    "#     point2_lon = longitudes + incr_lon / 2\n",
    "\n",
    "#     # Calculer les distances (largeur et longueur)\n",
    "#     largeur = round(vectorized_haversine_formula(origine_lat, origine_lon, point1_lat, origine_lon),3)\n",
    "#     longueur = round(vectorized_haversine_formula(origine_lat, origine_lon, origine_lat, point2_lon),3)\n",
    "#     return np.column_stack((largeur, longueur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60922ea-a75f-4633-b06c-9d6276b9e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# date = '2020-08-01'\n",
    "# df = filtered_data[filtered_data[\"DATE\"]==date]\n",
    "# df = df[[\"CELL_LAT\",\"CELL_LON\",\"COORDINATES_LAT\",\"COORDINATES_LON\"]]\n",
    "# values = np.arange(0, 1.1, 0.1)\n",
    "\n",
    "# # Probabilités : 0 a une probabilité de 0.2, les autres valeurs se partagent 0.8 de manière uniforme\n",
    "# probs = [0.2] + [0.8 / (len(values) - 1)] * (len(values) - 1)\n",
    "\n",
    "# # Génération des valeurs aléatoires avec les probabilités définies\n",
    "# df[\"IS_FIRE\"] = np.round(np.random.choice(values, size=len(df), p=probs),1)\n",
    "# df[[\"CELL_WIDTH\",\"CELL_LENGTH\"]] = vectorized_cell_width_length(df[\"CELL_LAT\"],df[\"CELL_LON\"],0.25,0.25)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d80a9-f34b-4459-aae8-fab7823c5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"output.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0adae-0a04-4063-80f7-af0988c4a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_lat = df[\"COORDINATES_LAT\"].max()\n",
    "# max_lon = df[\"COORDINATES_LON\"].max()\n",
    "\n",
    "# print(\"Max CELL_LAT:\", max_lat)\n",
    "# print(\"Max CELL_LON:\", max_lon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60dccd4-8b7e-465d-a81f-56f5a97b02d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b2b208-8688-4780-86e9-5d4a2dfdb058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pasqal)",
   "language": "python",
   "name": "pasqal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
